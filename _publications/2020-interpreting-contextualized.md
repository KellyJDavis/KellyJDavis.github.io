---
title: "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings"
collection: publications
category: conferences
permalink: /publication/2020-interpreting-contextualized
excerpt: 'Methods for converting contextualized representations (e.g. ELMo, BERT) to static lookup-table embeddings; analysis of representational quality and social biases across pretrained models.'
date: 2020-01-01
venue: 'ACL 2020'
paperurl: 'https://aclanthology.org/2020.acl-main.431/'
citation: 'Rishi Bommasani, Kelly Davis, Claire Cardie. (2020). &quot;Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings.&quot; <i>ACL 2020</i>.'
---

Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP. We introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings, applied to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. We also consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find dramatic inconsistencies between social bias estimators for word embeddings.
